---
title: Query Panel
description: Interactive query execution for testing retrieval.
---

The Query panel lets you run retrieval queries interactively against your vector store. This is invaluable for testing whether your ingested content is retrievable, understanding why certain results rank higher than others, and iterating on query phrasing without modifying your application code.

## Setup requirements

The Query panel requires that your engine is registered with the debug runtime:

```ts
import { registerUnragDebug } from "@unrag/debug";

registerUnragDebug({ engine });
```

Without registration, the panel displays a warning about missing query capability. The debug server must advertise the `query` capability for this panel to function.

## Running a query

The Query panel has an input section at the top where you configure your query:

| Field | Description |
|-------|-------------|
| **query** | The search text to embed and match against your vectors |
| **scope** | Optional filter prefix (currently display-only) |
| **topK** | Number of results to return (adjustable with `+`/`-`) |

To execute a query:

1. Press `e` to enter edit mode for the query text
2. Type your query
3. Press `Enter` or `Escape` to exit edit mode
4. Press `r` to run the query

## Keyboard shortcuts

| Key | Action |
|-----|--------|
| `e` | Edit the query text |
| `r` | Run the query |
| `+` | Increase topK (max 50) |
| `-` | Decrease topK (min 1) |
| `j` or `↓` | Navigate results down |
| `k` or `↑` | Navigate results up |
| `PageUp` / `Ctrl+U` | Scroll content up |
| `PageDown` / `Ctrl+D` | Scroll content down |

## Understanding results

The results panel shows matching chunks with:

| Column | Description |
|--------|-------------|
| **Score** | Similarity score (higher = more relevant) |
| **Source ID** | The document's source identifier |
| **Doc ID** | Internal document ID |
| **Content** | Preview of the chunk content |

Scores are typically between 0 and 1 for cosine similarity. A score of 0.85 or higher usually indicates strong relevance, but the exact threshold depends on your embedding model and content type.

Navigate to a result with `j`/`k` to select it and see full details in the right panel.

## Details panel

When a result is selected, the details panel shows:

- **source** — Full source ID (not truncated)
- **doc** — Full document ID
- **score** — Precise score to 6 decimal places
- **content** — Full chunk content (scrollable with `PageUp`/`PageDown`)

This lets you read the complete chunk text that would be passed to your LLM, helping you understand exactly what context your application receives.

## Timing breakdown

After a query completes, the header shows timing information:

- **total** — End-to-end query time
- **embed** — Time to generate the query embedding
- **db** — Time for the database similarity search

If queries feel slow, this breakdown identifies whether embedding or database search is the bottleneck.

## Typical workflow

1. **Run a test query** for something you know should exist in your data
2. **Check if the expected result appears** in the top results
3. **If not, examine what did appear** — Are these semantically similar? Is there a vocabulary mismatch?
4. **Try different query phrasings** to understand how the embedding model interprets different formulations
5. **Adjust topK** to see if the expected result appears lower in the ranking

## What's next

<Cards>
  <Card title="Docs Panel" href="/docs/debugging/panels/docs">
    Inspect the documents and chunks in your store
  </Card>
  <Card title="Best Practices" href="/docs/debugging/best-practices">
    Effective workflows for debugging retrieval
  </Card>
</Cards>

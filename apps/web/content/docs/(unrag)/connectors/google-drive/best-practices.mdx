---
title: Best Practices
description: Production patterns for Google Drive ingestion with Unrag.
---

The Google Drive connector is designed to be safe and idempotent, but production deployments benefit from a few patterns that handle edge cases gracefully.

## Choose the right authentication model

The authentication model you choose affects both security and operational complexity.

**OAuth2** is the right choice when users connect their own accounts. Each user controls which files they share, and you store their refresh tokens. This model is more work to implement (you need the OAuth consent flow), but it's the only option for consumer apps or situations where you can't guarantee access to files upfront.

**Service accounts with explicit sharing** work well for internal tools or small-scale deployments. Share files and folders with the service account's email, and it can access them directly. This is simple but doesn't scale if you need access to many files across different owners.

**Service accounts with domain-wide delegation (DWD)** are the most powerful option for organizations on Google Workspace. The service account can impersonate any user in your domain, accessing their files without explicit sharing. This is ideal for org-wide knowledge bases or backups. The tradeoff is setup complexity—DWD requires Workspace admin configuration and is only available for Workspace accounts, not consumer Gmail.

If you're building for an organization with Workspace, start with DWD. It's more work upfront but eliminates the "someone forgot to share the folder" failure mode.

## Handle token refresh gracefully

OAuth refresh tokens can expire or be revoked. When a user's token stops working, your sync will fail with 401 errors. Build your application to handle this:

```ts
try {
  await syncGoogleDriveFiles({
    engine,
    auth: { kind: "oauth", clientId, clientSecret, redirectUri, refreshToken },
    fileIds,
  });
} catch (err) {
  if (isTokenExpiredError(err)) {
    // Prompt user to re-authenticate
    await markUserNeedsReauth(userId);
    return { success: false, reason: "auth_expired" };
  }
  throw err;
}
```

Service account credentials don't expire in the same way, but the JSON key can be rotated or deleted. If you rotate keys, make sure your deployment picks up the new credentials.

## Use namespace prefixes for multi-tenant apps

If your application serves multiple tenants, use `sourceIdPrefix` to partition content:

```ts
await syncGoogleDriveFiles({
  engine,
  auth,
  fileIds,
  sourceIdPrefix: `tenant:${tenantId}:`,
});
```

This makes retrieval scoping simple and prevents accidental cross-tenant data leakage. When a tenant disconnects or deletes their account, you can cleanly wipe their content:

```ts
await engine.delete({ sourceIdPrefix: `tenant:${tenantId}:` });
```

## Track which files to sync

The connector ingests specific files by ID—it doesn't crawl folders or discover new files automatically. You need a way to track which files should be synced.

For small, static file sets, a configuration file or environment variable works fine:

```ts
const fileIds = process.env.DRIVE_FILE_IDS?.split(",") ?? [];
```

For larger or dynamic sets, store file IDs in your database. When users add files to sync, record the IDs. Your sync job then queries the database for the current list.

If you need folder-level syncing, use the Drive API to list files in a folder before syncing:

```ts
import { createGoogleDriveClient } from "@unrag/connectors/google-drive";

const { drive } = await createGoogleDriveClient({ auth });

const files: string[] = [];
let pageToken: string | undefined;

do {
  const res = await drive.files.list({
    q: `'${folderId}' in parents and trashed = false`,
    fields: "nextPageToken, files(id)",
    pageSize: 100,
    pageToken,
    supportsAllDrives: true,
    includeItemsFromAllDrives: true,
  });
  
  files.push(...(res.data.files?.map((f: any) => f.id) ?? []));
  pageToken = res.data.nextPageToken ?? undefined;
} while (pageToken);

await syncGoogleDriveFiles({ engine, auth, fileIds: files });
```

## Enable `deleteOnNotFound` for full sync

If you maintain a list of file IDs and want your index to stay in sync when files are deleted or permissions are revoked, enable `deleteOnNotFound`:

```ts
await syncGoogleDriveFiles({
  engine,
  auth,
  fileIds: getCurrentFileIds(),
  deleteOnNotFound: true,
});
```

When the connector can't access a file (404 or 403), it removes the corresponding document from your store. This keeps your index clean without requiring manual cleanup.

## Set appropriate file size limits

The default `maxBytesPerFile` is 15MB, which is reasonable for most documents. If you're ingesting large PDFs or media files, you might want to increase it:

```ts
await syncGoogleDriveFiles({
  engine,
  auth,
  fileIds,
  options: {
    maxBytesPerFile: 50 * 1024 * 1024, // 50MB
  },
});
```

But be thoughtful about this. Large files take longer to download, cost more to process (especially for LLM extraction), and may produce many chunks. Consider whether you actually need to ingest huge files, or whether a size limit that skips them is the right behavior.

## Run sync in background jobs

For production deployments, don't run sync in request handlers. File downloads, exports, and ingestion can be slow, and you don't want to block user-facing requests or risk timeouts.

Instead, run sync from background jobs: cron scripts, BullMQ workers, Inngest functions, QStash schedules, or similar. This gives you:

- **Retries**: If a sync fails partway through, you can retry without losing progress
- **Observability**: Job runners typically provide logging, metrics, and alerting
- **Rate limit handling**: You can add delays between files to avoid hitting Google's rate limits
- **Timeout safety**: Background jobs can run longer than HTTP request timeouts

See the [Next.js Production Recipe](/docs/guides/nextjs-production-recipe) for patterns that work well on Vercel.

## Batch large file lists

If you're syncing hundreds or thousands of files, batch your calls and add pauses to stay within Google's rate limits:

```ts
const BATCH_SIZE = 20;
const PAUSE_MS = 2000;

for (let i = 0; i < allFileIds.length; i += BATCH_SIZE) {
  const batch = allFileIds.slice(i, i + BATCH_SIZE);
  
  await syncGoogleDriveFiles({
    engine,
    auth,
    fileIds: batch,
  });
  
  if (i + BATCH_SIZE < allFileIds.length) {
    await new Promise((r) => setTimeout(r, PAUSE_MS));
  }
}
```

Google Drive has quota limits that vary by API and account type. For most use cases, 20 files with a 2-second pause between batches keeps you safely under the limits.

## Monitor for asset processing warnings

The connector downloads files and emits them as `assets` for the engine to process. If asset processing fails or is disabled for certain file types, you might silently miss content.

Always check the ingest result for warnings when using the lower-level API:

```ts
const result = await engine.ingest({
  sourceId: doc.sourceId,
  content: doc.content,
  assets: doc.assets,
  metadata: doc.metadata,
});

if (result.warnings.length > 0) {
  console.warn("Asset processing warnings:", result.warnings);
  // Forward to your monitoring system
}
```

The high-level `syncGoogleDriveFiles` function doesn't expose per-file warnings in the return value (they're handled internally during ingest), but you can use `onProgress` to observe skipped files:

```ts
await syncGoogleDriveFiles({
  engine,
  auth,
  fileIds,
  onProgress: (event) => {
    if (event.type === "file:skipped") {
      console.warn(`Skipped ${event.fileId}: ${event.message}`);
    }
  },
});
```

## Test with a small file set first

Before running sync on your full file list, test with 2-3 files to verify:

- Authentication works (correct credentials, proper permissions)
- File types are handled as expected (Google Docs export to text, PDFs emit as assets)
- Chunking and embedding produce reasonable results
- Your asset processing config handles the file types you have

Once the small test works, scale up gradually. This catches configuration issues early before you've spent time and API credits on a large sync.


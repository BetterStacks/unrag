---
title: Best Practices
description: Production patterns for Google Drive ingestion with Unrag.
---

The Google Drive connector is designed to be safe and idempotent, but production deployments benefit from a few patterns that handle edge cases gracefully.

## Choose the right authentication model

The authentication model you choose affects both security and operational complexity.

**OAuth2** is the right choice when users connect their own accounts. Each user controls which files they share, and you store their refresh tokens. This model is more work to implement (you need the OAuth consent flow), but it's the only option for consumer apps or situations where you can't guarantee access to files upfront.

**Service accounts with explicit sharing** work well for internal tools or small-scale deployments. Share files and folders with the service account's email, and it can access them directly. This is simple but doesn't scale if you need access to many files across different owners.

**Service accounts with domain-wide delegation (DWD)** are the most powerful option for organizations on Google Workspace. The service account can impersonate any user in your domain, accessing their files without explicit sharing. This is ideal for org-wide knowledge bases or backups. The tradeoff is setup complexity—DWD requires Workspace admin configuration and is only available for Workspace accounts, not consumer Gmail.

If you're building for an organization with Workspace, start with DWD. It's more work upfront but eliminates the "someone forgot to share the folder" failure mode.

## Handle token refresh gracefully

OAuth refresh tokens can expire or be revoked. When a user's token stops working, your sync will fail with 401 errors. Build your application to handle this:

```ts
try {
  const stream = googleDriveConnector.streamFiles({
    auth: { kind: "oauth", clientId, clientSecret, redirectUri, refreshToken },
    fileIds,
  });

  await engine.runConnectorStream({ stream });
} catch (err) {
  if (isTokenExpiredError(err)) {
    // Prompt user to re-authenticate
    await markUserNeedsReauth(userId);
    return { success: false, reason: "auth_expired" };
  }
  throw err;
}
```

Service account credentials don't expire in the same way, but the JSON key can be rotated or deleted. If you rotate keys, make sure your deployment picks up the new credentials.

## Use namespace prefixes for multi-tenant apps

If your application serves multiple tenants, use `sourceIdPrefix` to partition content:

```ts
const stream = googleDriveConnector.streamFiles({
  auth,
  fileIds,
  sourceIdPrefix: `tenant:${tenantId}:`,
});

await engine.runConnectorStream({ stream });
```

This makes retrieval scoping simple and prevents accidental cross-tenant data leakage. When a tenant disconnects or deletes their account, you can cleanly wipe their content:

```ts
await engine.delete({ sourceIdPrefix: `tenant:${tenantId}:` });
```

## Track which files to sync

The connector ingests specific files by ID—it doesn't crawl folders or discover new files automatically. You need a way to track which files should be synced.

For small, static file sets, a configuration file or environment variable works fine:

```ts
const fileIds = process.env.DRIVE_FILE_IDS?.split(",") ?? [];
```

For larger or dynamic sets, store file IDs in your database. When users add files to sync, record the IDs. Your sync job then queries the database for the current list.

If you need folder-level syncing, use the Drive API to list files in a folder before syncing:

```ts
import { createGoogleDriveClient, googleDriveConnector } from "@unrag/connectors/google-drive";

const { drive } = await createGoogleDriveClient({ auth });

const files: string[] = [];
let pageToken: string | undefined;

do {
  const res = await drive.files.list({
    q: `'${folderId}' in parents and trashed = false`,
    fields: "nextPageToken, files(id)",
    pageSize: 100,
    pageToken,
    supportsAllDrives: true,
    includeItemsFromAllDrives: true,
  });
  
  files.push(...(res.data.files?.map((f: any) => f.id) ?? []));
  pageToken = res.data.nextPageToken ?? undefined;
} while (pageToken);

const stream = googleDriveConnector.streamFiles({ auth, fileIds: files });
await engine.runConnectorStream({ stream });
```

## Enable deleteOnNotFound for full sync

If you maintain a list of file IDs and want your index to stay in sync when files are deleted or permissions are revoked, enable `deleteOnNotFound`:

```ts
const stream = googleDriveConnector.streamFiles({
  auth,
  fileIds: getCurrentFileIds(),
  deleteOnNotFound: true,
});

await engine.runConnectorStream({ stream });
```

When the connector can't access a file (404 or 403), it emits a delete event that removes the corresponding document from your store. This keeps your index clean without requiring manual cleanup.

## Set appropriate file size limits

The default `maxBytesPerFile` is 15MB, which is reasonable for most documents. If you're ingesting large PDFs or media files, you might want to increase it:

```ts
const stream = googleDriveConnector.streamFiles({
  auth,
  fileIds,
  options: {
    maxBytesPerFile: 50 * 1024 * 1024, // 50MB
  },
});

await engine.runConnectorStream({ stream });
```

But be thoughtful about this. Large files take longer to download, cost more to process (especially for LLM extraction), and may produce many chunks. Consider whether you actually need to ingest huge files, or whether a size limit that skips them is the right behavior.

## Persist checkpoints for large syncs

For syncs with many files—especially in serverless environments with timeout limits—persist checkpoints so you can resume if interrupted:

```ts
const stream = googleDriveConnector.streamFiles({
  auth,
  fileIds: largeFileList,
  checkpoint: await loadLastCheckpoint(tenantId),
});

await engine.runConnectorStream({
  stream,
  onCheckpoint: async (checkpoint) => {
    await saveCheckpoint(tenantId, checkpoint);
  },
});
```

Each checkpoint includes the index of the next file to process. If the sync times out, the next invocation picks up exactly where it left off.

## Run sync in background jobs

For production deployments, don't run sync in request handlers. File downloads, exports, and ingestion can be slow, and you don't want to block user-facing requests or risk timeouts.

Instead, run sync from background jobs: cron scripts, BullMQ workers, Inngest functions, QStash schedules, or similar. This gives you:

- **Retries**: If a sync fails partway through, you can retry without losing progress
- **Observability**: Job runners typically provide logging, metrics, and alerting
- **Rate limit handling**: You can add delays between files to avoid hitting Google's rate limits
- **Timeout safety**: Background jobs can run longer than HTTP request timeouts

See the [Next.js Production Recipe](/docs/guides/nextjs-production-recipe) for patterns that work well on Vercel.

## Batch large file lists

If you're syncing hundreds or thousands of files, batch your calls and add pauses to stay within Google's rate limits:

```ts
const BATCH_SIZE = 20;
const PAUSE_MS = 2000;

for (let i = 0; i < allFileIds.length; i += BATCH_SIZE) {
  const batch = allFileIds.slice(i, i + BATCH_SIZE);
  
  const stream = googleDriveConnector.streamFiles({
    auth,
    fileIds: batch,
  });

  await engine.runConnectorStream({ stream });
  
  if (i + BATCH_SIZE < allFileIds.length) {
    await new Promise((r) => setTimeout(r, PAUSE_MS));
  }
}
```

Google Drive has quota limits that vary by API and account type. For most use cases, 20 files with a 2-second pause between batches keeps you safely under the limits.

## Use onEvent for observability

The streaming model makes it easy to log exactly what's happening during a sync:

```ts
await engine.runConnectorStream({
  stream,
  onEvent: (event) => {
    if (event.type === "progress") {
      console.log(`[${event.current}/${event.total}] ${event.message}`);
    }
    if (event.type === "warning") {
      console.warn(`Warning: [${event.code}] ${event.message}`);
    }
  },
});
```

Forward these events to your logging/monitoring system to catch issues early.

## Monitor for asset processing warnings

The connector downloads files and emits them as `assets` for the engine to process. If asset processing fails or is disabled for certain file types, you might silently miss content.

Always check the ingest result for warnings when using the lower-level API:

```ts
const result = await engine.ingest({
  sourceId: doc.sourceId,
  content: doc.content,
  assets: doc.assets,
  metadata: doc.metadata,
});

if (result.warnings.length > 0) {
  console.warn("Asset processing warnings:", result.warnings);
  // Forward to your monitoring system
}
```

The high-level streaming API handles this internally during ingest, but you can use `onEvent` to observe skipped files:

```ts
await engine.runConnectorStream({
  stream,
  onEvent: (event) => {
    if (event.type === "warning" && event.code === "file_skipped") {
      console.warn(`Skipped ${event.data?.fileId}: ${event.message}`);
    }
  },
});
```

## Test with a small file set first

Before running sync on your full file list, test with 2-3 files to verify:

- Authentication works (correct credentials, proper permissions)
- File types are handled as expected (Google Docs export to text, PDFs emit as assets)
- Chunking and embedding produce reasonable results
- Your asset processing config handles the file types you have

Once the small test works, scale up gradually. This catches configuration issues early before you've spent time and API credits on a large sync.

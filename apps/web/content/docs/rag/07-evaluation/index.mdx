---
title: Module 7 — Evaluation
description: Measure retrieval and answer quality, build datasets, run regressions, and avoid evaluation traps.

---

<Callout type="warn" title="Status: Draft">
This module is a scaffold. The goal is to make evaluation feel like an engineering workflow, not a research project.
</Callout>

## What you’ll learn

- What to measure (retrieval vs generation) and what not to over-index on
- How to build datasets that reflect your users and your risk profile
- How to run offline regression tests and catch quality backslides
- How to design online evaluation and feedback loops

## Chapters

<Cards>
  <Card title="1. What to measure" href="/docs/rag/07-evaluation/01-what-to-measure" />
  <Card title="2. Building eval datasets" href="/docs/rag/07-evaluation/02-building-eval-datasets" />
  <Card title="3. Offline evals and regression testing" href="/docs/rag/07-evaluation/03-offline-evals-and-regression-testing" />
  <Card title="4. Online evaluation and feedback loops" href="/docs/rag/07-evaluation/04-online-evaluation-and-feedback-loops" />
  <Card title="5. LLM-as-judge and rubrics" href="/docs/rag/07-evaluation/05-llm-as-judge-and-rubrics" />
  <Card title="6. Debugging with eval slices" href="/docs/rag/07-evaluation/06-debugging-with-eval-slices" />
</Cards>

## Next

<Cards>
  <Card title="Next: What to measure" href="/docs/rag/07-evaluation/01-what-to-measure" />
</Cards>


---
title: Image Extractors
description: Make images searchable through direct embedding or caption-based retrieval.
---

Images are everywhere in knowledge bases—diagrams in documentation, screenshots in support tickets, product photos in catalogs. Making them searchable opens up powerful retrieval capabilities. A user asking "what does the architecture look like?" can find that system diagram. A support agent searching "error message" can surface screenshots showing the actual error.

Unrag supports two fundamentally different approaches to image search, and you can use both together.

## Two ways to search images

### Direct embedding (image:embed)

With a multimodal embedding model, you can embed images directly into the same vector space as text. When someone searches "colorful sunset", the query embedding is compared directly against image embeddings—no text required.

This is powerful for visual similarity search. A query about "system architecture" might surface a diagram even if the diagram has no text labels.

### Text-based search (captions, OCR, descriptions)

The alternative is to convert images to text, then embed that text. This works with any embedding model and has its own advantages—you can add context that isn't visible in the image itself ("photo from the 2024 all-hands meeting").

Text can come from:
- **Provided captions** — alt text, descriptions from your source system
- **OCR** — extracted text from screenshots, charts, UI captures
- **LLM-generated captions** — AI-written descriptions of what the image shows

## Available image extractors

| Extractor | How it works | Best for |
|-----------|--------------|----------|
| image:embed | Embeds image pixels directly | Visual similarity, diagrams, photos |
| image:caption | Embeds provided caption/alt text | When good descriptions already exist |
| [image:ocr](/docs/extractors/image/ocr) | OCR images into text chunks | Screenshots, charts, UI captures |
| [image:caption-llm](/docs/extractors/image/caption-llm) | Generates captions via LLM | Images without source captions |

## How Unrag chooses

During ingestion, Unrag decides how to handle each image based on your configuration:

1. **If your embedding provider has `embedImage()`** → produces an image embedding (image:embed)
2. **If the asset has `text` (caption)** → also produces a text chunk (image:caption)
3. **If image extractors are installed and enabled** → may produce additional text chunks (OCR, LLM captions)
4. **If nothing produces output** → skips the image with a warning

This means you can combine approaches. With a multimodal embedding model and `image:ocr` installed, an image might produce both:
- A direct image embedding (visual search)
- A text chunk with OCR content (text search for words in the image)

## Choosing your approach

### Use multimodal embedding when:

- Visual similarity matters ("find images like this one")
- Images are diagrams, photos, or visual content
- You want queries to find images by what they show, not how they're described

Configure a multimodal model:

```ts
const embedding = createAiEmbeddingProvider({
  type: "multimodal",
  model: "cohere/embed-v4.0",
});
```

### Use text-based extraction when:

- You're using a text-only embedding model
- Good captions already exist in your source data
- You want to add context the image doesn't show
- Text in the image is what matters (screenshots, charts)

Configure a text-only model (images fall back to captions/extractors):

```ts
const embedding = createAiEmbeddingProvider({
  type: "text",
  model: "openai/text-embedding-3-small",
});
```

## Providing captions

When captions exist in your source data (Notion image captions, CMS alt text, etc.), pass them via the `text` field:

```ts
await engine.ingest({
  sourceId: "docs:architecture",
  content: "System Architecture Overview",
  assets: [
    {
      assetId: "arch-diagram",
      kind: "image",
      data: { kind: "url", url: "https://..." },
      text: "High-level architecture diagram showing frontend, API gateway, and microservices",
    },
  ],
});
```

This caption becomes searchable. Queries for "microservices" will find this image even if the diagram doesn't have that word visible.

### Writing good captions

Good captions describe:

- **What the image shows** — objects, people, scenes, diagrams
- **Context not visible** — product name, location, date, event
- **Why it matters** — "illustrates the payment flow", "shows step 3 of setup"

Bad caption: "screenshot.png"
Good caption: "Error dialog showing 'Connection timeout' message with retry button in the mobile app settings screen"

## Retrieving image chunks

Image chunks are retrieved like any other content. Use `getChunkAssetRef()` to identify them:

```ts
import { getChunkAssetRef } from "@unrag/core";

const result = await engine.retrieve({ query: "product photos" });

for (const chunk of result.chunks) {
  const ref = getChunkAssetRef(chunk);
  
  if (ref?.assetKind === "image") {
    console.log(`Image match (via ${ref.extractor}):`);
    console.log(`  Asset ID: ${ref.assetId}`);
    
    // For caption/OCR chunks, content is the text
    if (chunk.content) {
      console.log(`  Text: ${chunk.content}`);
    }
    
    // Get the image URL for display
    if (ref.assetUri) {
      console.log(`  URL: ${ref.assetUri}`);
    }
  }
}
```

## Combining approaches

For best results, combine multiple approaches. With a multimodal model and `image:ocr`:

```ts
export const unrag = defineUnragConfig({
  // ...
  engine: {
    embedding: createAiEmbeddingProvider({
      type: "multimodal",
      model: "cohere/embed-v4.0",
    }),
    extractors: [createImageOcrExtractor()],
    assetProcessing: {
      image: {
        ocr: { enabled: true },
      },
    },
  },
} as const);
```

Now when you ingest a screenshot:
- It gets a direct image embedding (finds it when searching "dialog box")
- It also produces OCR text chunks (finds it when searching "Connection timeout")

You get the best of both worlds.

## Image size limits

Be mindful of image sizes. Large images consume memory and API bandwidth. Configure limits:

```ts
assetProcessing: {
  fetch: {
    maxBytes: 10 * 1024 * 1024,  // 10MB max for any asset
  },
  image: {
    ocr: {
      maxBytes: 5 * 1024 * 1024,  // 5MB max for OCR
    },
  },
},
```

Images exceeding these limits are skipped with warnings.

